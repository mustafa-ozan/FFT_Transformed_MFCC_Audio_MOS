{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"88GNi-OJZNPC"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiYgOqDfBINZ"},"outputs":[],"source":["# 14.12.2025\n","# NN models definitions, training and testing processes for VoiceMOS 2022 data\n","#\n","# Author : Mustafa Ozan Duman\n","\n","import torch.nn as nn\n","import torch\n","from tqdm.notebook import tqdm\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","from scipy.stats import pearsonr, spearmanr\n","import os\n","import shutil\n","\n","# --- Hyperparameters (You can adjust these later) ---\n","BATCH_SIZE = 32\n","LEARNING_RATE = 0.001\n","NUM_EPOCHS = 50\n","\n","# --- Feature Dimensions ---\n","# The input shape of the feature matrix (excluding the batch dimension)\n","INPUT_ROWS = 2288\n","INPUT_COLS = 20\n","OUTPUT_DIM = 1 # For MOS prediction (a continuous score)\n","\n","\n","# model 1 classical CNN + MLP\n","\n","class VoiceMOS_CNN(nn.Module):\n","    def __init__(self):\n","        super(VoiceMOS_CNN, self).__init__()\n","\n","        # We start with 1 input channel (since it's a magnitude matrix)\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Calculate the size of the feature map after convolutional and pooling layers\n","        # Input size: 2288 x 20\n","        # After conv1 (MaxPool 2x2): floor(2288/2) x floor(20/2) = 1144 x 10\n","        # After conv2 (MaxPool 2x2): floor(1144/2) x floor(10/2) = 572 x 5\n","        # Output channels from conv2: 32\n","\n","        # Flattened size: 32 channels * 572 rows * 5 columns\n","        self.flattened_size = 32 * 572 * 5\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.flattened_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, OUTPUT_DIM) # Final output is a single MOS score\n","        )\n","\n","    def forward(self, x):\n","        # x shape: (Batch_Size, 1, 2288, 20)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = x.view(x.size(0), -1) # Flatten\n","        x = self.fc(x)\n","        return x\n","\n","\n","\n","# model 2 resnet\n","\n","class VoiceMOS_ResNet(nn.Module):\n","    \"\"\"\n","    ResNet-18 architecture adapted for single-channel (1, 2288, 20) input\n","    and single-score MOS regression output.\n","    \"\"\"\n","    def __init__(self, use_pretrained=True):\n","        super(VoiceMOS_ResNet, self).__init__()\n","\n","        # Load weights: Use pre-trained weights to leverage learned features\n","        # from ImageNet, or set to None for training from scratch.\n","        weights = ResNet18_Weights.IMAGENET1K_V1 if use_pretrained else None\n","        self.resnet = resnet18(weights=weights)\n","\n","        # 1. Modify the first convolutional layer (Input Layer)\n","        # Change in_channels from 3 to 1 to accept your single-channel FFT matrix.\n","        self.resnet.conv1 = nn.Conv2d(\n","            in_channels=1,\n","            out_channels=64,\n","            kernel_size=7,\n","            stride=2,\n","            padding=3,\n","            bias=False\n","        )\n","\n","        # 2. Modify the last fully connected layer (Output Layer)\n","        # Get the size of the features coming out of the average pooling layer (512 for ResNet18)\n","        num_ftrs = self.resnet.fc.in_features\n","\n","        # Replace the final classification layer with a regression layer (output 1 score)\n","        self.resnet.fc = nn.Linear(num_ftrs, 1)\n","\n","    def forward(self, x):\n","        # x shape: (Batch_Size, 1, 2288, 20)\n","        # ResNet performs the forward pass directly.\n","        return self.resnet(x)\n","\n","\n","\n","# model 3 CNN-Bidirectional LSTM Hybridm\n","\n","class VoiceMOS_LSTM(nn.Module):\n","    \"\"\"\n","    CNN-LSTM Hybrid for MOS Prediction.\n","    Treats the (2288, 20) feature matrix as a sequence of 2288 time steps,\n","    each having 20 features.\n","    \"\"\"\n","    def __init__(self, input_size=20, hidden_size=128, num_layers=2, bidirectional=True):\n","        super(VoiceMOS_LSTM, self).__init__()\n","\n","        # Hyperparameters\n","        self.D = 2 if bidirectional else 1\n","        self.H = hidden_size\n","\n","        # 1. Bidirectional LSTM Layer\n","        # Processes the time sequence (2288 steps)\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,  # Input shape: (Batch, Seq_Len, Feature_Dim)\n","            bidirectional=bidirectional\n","        )\n","\n","        # 2. Aggregation and Prediction Layer\n","        # Input size is D * H (e.g., 2 * 128 = 256) after averaging the sequence.\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.D * self.H, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1) # Output single MOS score\n","        )\n","\n","    def forward(self, x):\n","        # x shape: (B, 1, 2288, 20)\n","\n","        # Squeeze channel dim and transpose to LSTM input format\n","        # Desired shape: (B, 2288, 20) -> (Batch, Sequence Length, Input Features)\n","        x = x.squeeze(1)\n","\n","        # Pass through LSTM\n","        # lstm_out shape: (B, 2288, D*H)\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Global Average Pooling (GAP) over the time dimension (dim=1)\n","        # to compress the entire utterance's sequence into a single vector: (B, D*H)\n","        avg_pool = torch.mean(lstm_out, dim=1)\n","\n","        # Final prediction\n","        return self.fc(avg_pool)\n","\n","\n","# Model 4: CNN-Bidirectional GRU Hybrid\n","\n","class VoiceMOS_GRU(nn.Module):\n","    \"\"\"\n","    CNN-GRU Hybrid Model (Uses GRU instead of LSTM).\n","    \"\"\"\n","    def __init__(self, input_size=20, hidden_size=128, num_layers=2, bidirectional=True):\n","        super(VoiceMOS_GRU, self).__init__()\n","\n","        self.D = 2 if bidirectional else 1\n","        self.H = hidden_size\n","\n","        # --- Only change is here: nn.GRU instead of nn.LSTM ---\n","        self.gru = nn.GRU(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=bidirectional\n","        )\n","        # -----------------------------------------------------\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.D * self.H, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        x = x.squeeze(1) # Shape: (B, 2288, 20)\n","\n","        # GRU output: (B, 2288, D*H)\n","        gru_out, _ = self.gru(x)\n","\n","        # Global Average Pooling over the time dimension\n","        avg_pool = torch.mean(gru_out, dim=1)\n","\n","        return self.fc(avg_pool)\n","\n","\n","\n","# Model 5: Transformer Encoder\n","\n","import torch.nn as nn\n","import torch\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class VoiceMOS_Transformer(nn.Module):\n","    def __init__(self, d_model=20, nhead=10, num_layers=5):\n","        super(VoiceMOS_Transformer, self).__init__()\n","\n","        # Ensure d_model (feature dimension) is divisible by nhead (4)\n","        if d_model % nhead != 0:\n","            raise ValueError(\"d_model must be divisible by nhead\")\n","\n","        # 1. Positional Encoding is implicitly handled by the standard Transformer\n","        # structure, but we rely on the self-attention mechanism to learn sequential context.\n","\n","        # Define one Encoder Layer\n","        encoder_layer = TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            batch_first=False # We will use (Sequence, Batch, Features) format\n","        )\n","\n","        # Stack multiple layers\n","        self.transformer_encoder = TransformerEncoder(\n","            encoder_layer,\n","            num_layers=num_layers\n","        )\n","\n","        # 2. Prediction Head\n","        self.fc = nn.Sequential(\n","            nn.Linear(d_model, 64), # d_model is 20\n","            nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        # x shape: (B, 1, 2288, 20)\n","\n","        # 1. Prepare for Transformer: (Seq Length, Batch, Feature Dim)\n","        x = x.squeeze(1)        # Shape: (B, 2288, 20)\n","        x = x.permute(1, 0, 2)  # Shape: (2288, B, 20) -> (Seq_Len, Batch, Features)\n","\n","        # 2. Pass through Transformer Encoder\n","        # Output shape: (2288, B, 20)\n","        transformer_output = self.transformer_encoder(x)\n","\n","        # 3. Global Pooling: Average across the sequence dimension (dim=0)\n","        # Result shape: (B, 20)\n","        avg_pool = torch.mean(transformer_output, dim=0)\n","\n","        # 4. Final Prediction\n","        return self.fc(avg_pool)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jq2PCN5Rp7p5"},"outputs":[],"source":["class MOSFeatureDataset(Dataset):\n","    def __init__(self, features_path, mos_scores_path):\n","        # Load the feature and score arrays\n","        self.features = np.load(features_path, allow_pickle=True)\n","        self.mos_scores = np.load(mos_scores_path)\n","\n","    def __len__(self):\n","        return len(self.mos_scores)\n","\n","    def __getitem__(self, idx):\n","        # Features are (2288, 20), need to add a channel dimension for PyTorch CNNs\n","        # From (H, W) -> (C, H, W) => (1, 2288, 20)\n","        feature = torch.from_numpy(self.features[idx]).float().unsqueeze(0)\n","        score = torch.tensor(self.mos_scores[idx]).float()\n","        return feature, score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3x01p2_qHiC"},"outputs":[],"source":["# Define the paths where you saved your data\n","BASE_FEATURE_DIR = '/content/drive/MyDrive/BUU_PHD_THESIS/VoiceMOS_2022_features/'\n","\n","TRAIN_FEAT_DIR = os.path.join(BASE_FEATURE_DIR, 'VoiceMOS_2022_train_data_features')\n","VAL_FEAT_DIR = os.path.join(BASE_FEATURE_DIR, 'VoiceMOS_2022_validation_data_features')\n","TEST_FEAT_DIR = os.path.join(BASE_FEATURE_DIR, 'VoiceMOS_2022_test_data_features')\n","\n","CNN_MLP_CHECKPOINT_PATH = '/content/drive/MyDrive/BUU_PHD_THESIS/Voice_MOS_2022_data_trained_models/CNN_MLP_classic_mos_model.pth'\n","RESNET_CHECKPOINT_PATH = '/content/drive/MyDrive/BUU_PHD_THESIS/Voice_MOS_2022_data_trained_models/ResNet18_mos_model.pth'\n","LSTM_CHECKPOINT_PATH = '/content/drive/MyDrive/BUU_PHD_THESIS/Voice_MOS_2022_data_trained_models/LSTM_mos_model.pth'\n","GRU_CHECKPOINT_PATH = '/content/drive/MyDrive/BUU_PHD_THESIS/Voice_MOS_2022_data_trained_models/GRU_mos_model.pth'\n","TRANSFORMER_CHECKPOINT_PATH = '/content/drive/MyDrive/BUU_PHD_THESIS/Voice_MOS_2022_data_trained_models/TRANSFORMER_mos_model.pth'\n","\n","train_dataset = MOSFeatureDataset(\n","    features_path=os.path.join(TRAIN_FEAT_DIR, 'mfcc_features.npy'),\n","    mos_scores_path=os.path.join(TRAIN_FEAT_DIR, 'mos_scores.npy')\n",")\n","\n","val_dataset = MOSFeatureDataset(\n","    features_path=os.path.join(VAL_FEAT_DIR, 'mfcc_features.npy'),\n","    mos_scores_path=os.path.join(VAL_FEAT_DIR, 'mos_scores.npy')\n",")\n","\n","test_dataset = MOSFeatureDataset(\n","    features_path=os.path.join(TEST_FEAT_DIR, 'mfcc_features.npy'),\n","    mos_scores_path=os.path.join(TEST_FEAT_DIR, 'mos_scores.npy')\n",")\n","\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# # Initialize Model, Loss, and Optimizer\n","\n","# --- Model Setup ---\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the ResNet model\n","model_CNN_MLP = VoiceMOS_CNN().to(device)\n","model_ResNet = VoiceMOS_ResNet(use_pretrained=True).to(device)\n","model_LSTM = VoiceMOS_LSTM().to(device)\n","model_GRU = VoiceMOS_GRU().to(device)\n","model_Transformer = VoiceMOS_Transformer().to(device)\n","\n","\n","# Loss Function and Optimizer\n","criterion = nn.MSELoss()\n","\n","\n","# --- Early Stopping Hyperparameters ---\n","PATIENCE = 10      # Number of epochs to wait for improvement before stopping\n","MIN_DELTA = 0.0001 # Minimum change required to qualify as improvement\n","\n","\n","def train_model(model, optimizer, checkpoint_path, start_epoch, best_val_loss_start, wait_counter_start, local_path):\n","\n","    print(f\"\\n--- Starting Training on {device} from Epoch {start_epoch} ---\")\n","\n","    # Early Stopping Variables (use starting values)\n","    best_val_loss = best_val_loss_start\n","    wait_counter = wait_counter_start\n","\n","    for epoch in range(NUM_EPOCHS):\n","        # ... (A. Training Phase: uses 'model' and 'optimizer' arguments)\n","        model.train()\n","        train_loss = 0.0\n","\n","        for features, scores in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n","            features, scores = features.to(device), scores.to(device).unsqueeze(1)\n","\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            loss = criterion(outputs, scores)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        # ... (B. Validation Phase: uses 'model' argument)\n","        model.eval()\n","        val_loss = 0.0\n","\n","        with torch.no_grad():\n","            for features, scores in val_loader:\n","                features, scores = features.to(device), scores.to(device).unsqueeze(1)\n","                outputs = model(features)\n","                loss = criterion(outputs, scores)\n","                val_loss += loss.item()\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        # ... (C. Checkpointing and Early Stopping: uses 'checkpoint_path' argument)\n","        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n","        print(f\"  Training MSE: {avg_train_loss:.4f}\")\n","        print(f\"  Validation MSE: {avg_val_loss:.4f}\")\n","\n","        if avg_val_loss < best_val_loss - MIN_DELTA:\n","            best_val_loss = avg_val_loss\n","            wait_counter = 0\n","\n","            # Create a dictionary containing all necessary states\n","            checkpoint = {\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'best_val_loss': best_val_loss,\n","                'wait_counter': wait_counter\n","            }\n","\n","            # 2. Save to the fast local path first\n","            torch.save(checkpoint, local_path)\n","\n","             # 3. Copy the completed local file to Google Drive\n","            shutil.copyfile(local_path, checkpoint_path)\n","\n","            print(\"  >>> Checkpoint saved (best validation MSE) to Drive and Local <<<\")\n","\n","            # Use the passed checkpoint_path for saving\n","            # torch.save(checkpoint, checkpoint_path)\n","            # print(\"  >>> Checkpoint saved (best validation MSE) <<<\")\n","\n","        else:\n","            wait_counter += 1\n","            print(f\"  No improvement for {wait_counter}/{PATIENCE} epochs.\")\n","\n","            if wait_counter >= PATIENCE:\n","                print(f\"  --- Early Stopping! ---\")\n","                break\n","\n","\n","def test_model(model, test_loader, model_path):\n","    \"\"\"\n","    Evaluates the model on the test dataset.\n","    Loads model weights from the 'model_state_dict' key within the checkpoint dictionary.\n","    \"\"\"\n","    print(\"\\n--- Starting Testing on Test Dataset ---\")\n","\n","    # Load the best model weights from the specified path\n","    try:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Load the ENTIRE checkpoint dictionary\n","        checkpoint = torch.load(model_path, map_location=device)\n","\n","        # ðŸŽ¯ CRITICAL FIX: Extract the model weights from the dictionary\n","        state_dict = checkpoint['model_state_dict']\n","\n","        # Load weights into the currently initialized model architecture\n","        model.load_state_dict(state_dict)\n","        print(f\"Successfully loaded model weights from: {model_path}\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: Model checkpoint not found at {model_path}. Cannot test.\")\n","        return\n","    except Exception as e:\n","        # A specific check for the case where the file might be old and saved without the dictionary wrapper\n","        if \"Missing key(s)\" in str(e) and \"'model_state_dict'\" in str(e):\n","             print(\"\\nFATAL ERROR: The saved file is corrupted or not a valid structured checkpoint.\")\n","        else:\n","            print(f\"Error loading model state: {e}. Check if the model class matches the saved weights.\")\n","        return\n","\n","    model.eval() # Set model to evaluation mode\n","    all_predictions = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for features, scores in tqdm(test_loader, desc=\"Testing\"):\n","            features, scores = features.to(device), scores.to(device).unsqueeze(1)\n","\n","            outputs = model(features)\n","\n","            # Collect results\n","            all_predictions.extend(outputs.cpu().numpy().flatten())\n","            all_targets.extend(scores.cpu().numpy().flatten())\n","\n","    # Convert to numpy arrays for metric calculation\n","    all_predictions = np.array(all_predictions)\n","    all_targets = np.array(all_targets)\n","\n","    # Calculate Metrics\n","    test_mse = mean_squared_error(all_targets, all_predictions)\n","    test_rmse = np.sqrt(test_mse)\n","    pearson_r, _ = pearsonr(all_targets, all_predictions)\n","    spearman_r, _ = spearmanr(all_targets, all_predictions)\n","\n","    print(\"\\n=============================================\")\n","    print(\"        MOS PREDICTION TEST RESULTS\")\n","    print(\"=============================================\")\n","    print(f\"Model Tested: {type(model).__name__}\")\n","    print(f\"Final Test Samples: {len(all_targets)}\")\n","    print(f\"MSE (Mean Squared Error): {test_mse:.4f}\")\n","    print(f\"RMSE (Root Mean Squared Error): {test_rmse:.4f}\")\n","    print(f\"PCC (Pearson Correlation Coefficient): {pearson_r:.4f}\")\n","    print(f\"SCC (Spearman Rank Correlation): {spearman_r:.4f}\")\n","    print(\"=============================================\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATID8o0qF29P"},"outputs":[],"source":["\n","def load_checkpoint_and_resume( model, optimizer, checkpoint_path, local_path ):\n","\n","    \"\"\"Loads a checkpoint if it exists and updates model, optimizer, and training metrics.\"\"\"\n","    start_epoch = 0\n","    best_val_loss = float('inf')\n","    wait_counter = 0\n","\n","    # Prioritize loading the local copy, then fall back to the permanent Drive copy\n","    load_path = local_path if os.path.exists(local_path) else checkpoint_path\n","\n","    if os.path.exists(load_path):\n","        print(f\"Found checkpoint! Resuming training from {load_path}\")\n","        try:\n","            # We use 'load_path' for loading\n","            checkpoint = torch.load(load_path, map_location=device)\n","\n","            # Load model and optimizer states\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","            # Load training variables\n","            start_epoch = checkpoint['epoch'] + 1\n","            best_val_loss = checkpoint['best_val_loss']\n","            wait_counter = checkpoint['wait_counter']\n","\n","            print(f\"Resumed from Epoch {start_epoch}, Best Loss: {best_val_loss:.4f}\")\n","\n","        except Exception as e:\n","            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n","\n","    # After loading, ensure the local file is present for faster subsequent saving\n","    # If we loaded from Drive, copy it back to local for the next save.\n","    if os.path.exists(checkpoint_path) and not os.path.exists(local_path):\n","        print(\"Copying Drive checkpoint to local path for faster saving...\")\n","        shutil.copyfile(checkpoint_path, local_path)\n","\n","    return start_epoch, best_val_loss, wait_counter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wM3AwVXfMPx0"},"outputs":[],"source":["# CNN_MLP_CHECKPOINT_PATH\n","# RESNET_CHECKPOINT_PATH\n","# LSTM_CHECKPOINT_PATH\n","# GRU_CHECKPOINT_PATH\n","# TRANSFORMER_CHECKPOINT_PATH\n","\n","# model_CNN_MLP\n","# model_ResNet\n","# model_LSTM\n","# model_GRU\n","# model_Transformer\n","\n","\n","# --- Execution Variables ---\n","\n","# select model and checkpoint using above names\n","\n","selected_model = model_Transformer\n","selected_path = TRANSFORMER_CHECKPOINT_PATH\n","\n","local_save_path = '/content/temp_checkpoint.pth' # Use a unique name per model\n","\n","# --- Directory Creation Block (Must be before all training) ---\n","# os.path.dirname() extracts the directory part of the path:\n","destination_dir = os.path.dirname( selected_path )\n","\n","# os.makedirs creates the directory (folder):\n","os.makedirs(destination_dir, exist_ok=True)\n","print(f\"Destination directory checked/created: {destination_dir}\")\n","\n","# 1. Initialize Optimizer (needs to be done before loading the checkpoint)\n","optimizer = optim.Adam(selected_model.parameters(), lr=LEARNING_RATE)\n","\n","# 2. Load Checkpoint and get starting parameters\n","start_epoch, best_val_loss_loaded, wait_counter_loaded = load_checkpoint_and_resume(\n","    selected_model, optimizer, selected_path, local_save_path\n",")\n","\n","# 3. Start Training\n","train_model(\n","    selected_model,\n","    optimizer,\n","    selected_path,\n","    start_epoch,\n","    best_val_loss_loaded,\n","    wait_counter_loaded,\n","    local_save_path # Pass the local path for saving\n",")\n","\n","# 4. Test the model\n","# The test_model function loads from selected_path, which is the final saved Drive location.\n","test_model(selected_model, test_loader, selected_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"if-yfbEgIJVJ"},"outputs":[],"source":["torch.cuda.empty_cache()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1_hGl5_tm12SfNxYR0a_QrJ-6CV5gcuLf","timestamp":1765697360879},{"file_id":"1arQi3dsNn3eI4C8pmfXuJvi0PO8cGG-W","timestamp":1764310910309}],"mount_file_id":"1arQi3dsNn3eI4C8pmfXuJvi0PO8cGG-W","authorship_tag":"ABX9TyP+eOvY9dswxccebMe0uSA5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}